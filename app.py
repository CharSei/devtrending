import json
from pathlib import Path
from io import BytesIO

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
from scipy.sparse import hstack


# -----------------------------
# Deterministic Trend Engine
# -----------------------------

REQUIRED_FIELDS = [
    "Name (QE)",
    "Title (QE)",
    "Event Subcategory (EV)",
    "Event Defect Code (EV)",
    "Direct cause details (QE)",
    "Day of Created Date (QE)",
]

def _clean_text(x: str) -> str:
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return ""
    x = str(x)
    x = x.replace("\n", " ").replace("\r", " ").replace("\t", " ")
    x = " ".join(x.split())
    return x.strip()

def _norm_colname(c: str) -> str:
    return " ".join(str(c).strip().lower().replace("\n", " ").split())

def _map_headers(df: pd.DataFrame) -> pd.DataFrame:
    cols = list(df.columns)
    norm = {_norm_colname(c): c for c in cols}

    def pick(candidates):
        for cand in candidates:
            for n, orig in norm.items():
                if cand in n:
                    return orig
        return None

    col_name = pick(["name (qe)", "qe number", "qe#", "qe id", "qe-id", "event id", "qe"])
    col_title = pick(["title (qe)", "title", "short description", "beschreibung", "titel"])
    col_subcat = pick(["event subcategory", "subcategory", "sub category", "unterkategorie"])
    col_defect = pick(["event defect code", "defect code", "defect", "fehlercode", "code"])
    col_cause = pick(["direct cause details", "direct cause", "cause details", "cause", "ursache"])
    col_date = pick(["day of created date", "created date", "creation date", "date created", "created", "datum"])

    rename = {}
    if col_name: rename[col_name] = "Name (QE)"
    if col_title: rename[col_title] = "Title (QE)"
    if col_subcat: rename[col_subcat] = "Event Subcategory (EV)"
    if col_defect: rename[col_defect] = "Event Defect Code (EV)"
    if col_cause: rename[col_cause] = "Direct cause details (QE)"
    if col_date: rename[col_date] = "Day of Created Date (QE)"

    df = df.rename(columns=rename)

    # ensure required exist
    for f in REQUIRED_FIELDS:
        if f not in df.columns:
            df[f] = ""

    # clean values
    for f in REQUIRED_FIELDS:
        df[f] = df[f].apply(_clean_text)

    # drop empty rows (no QE id and no title)
    df = df[~((df["Name (QE)"] == "") & (df["Title (QE)"] == ""))].copy()
    return df

def _keywords_phrases(texts, top_k=6):
    """Deterministische Phrasenextraktion (Bigrams/Trigrams) fÃ¼r verstÃ¤ndliche Trendtitel."""
    stop = set([
        "und","oder","der","die","das","mit","auf","in","von","fÃ¼r","ist","eine","ein","bei",
        "wurde","werden","nicht","zu","als","aufgrund","im","am","an","aus","nach","vor","wÃ¤hrend",
        "the","and","or","of","to","in","on","for","with","is","are","was","were","issue","problem"
    ])
    from collections import Counter
    cnt = Counter()
    def tokens(t):
        t = _clean_text(t).lower().replace("/", " ")
        toks = []
        for w in t.split():
            w = "".join(ch for ch in w if ch.isalnum())
            if len(w) >= 4 and w not in stop:
                toks.append(w)
        return toks

    for t in texts:
        toks = tokens(t)
        for i in range(len(toks)-1):
            cnt[f"{toks[i]} {toks[i+1]}"] += 1
        for i in range(len(toks)-2):
            cnt[f"{toks[i]} {toks[i+1]} {toks[i+2]}"] += 1

    phrases = [p for p,_ in cnt.most_common(top_k)]
    if not phrases:
        c2 = Counter()
        for t in texts:
            for w in tokens(t):
                c2[w] += 1
        phrases = [w for w,_ in c2.most_common(top_k)]
    return phrases

def _trend_sentence(subcat, defect, titles, causes):
    """Trendname als klarer deutscher Satz."""
    phrases = _keywords_phrases(titles + causes, top_k=5)
    core = phrases[0] if phrases else "Ã¤hnliche Abweichungen"
    return f"In der Gruppe {subcat} â†’ {defect} treten wiederholt Abweichungen im Zusammenhang mit \"{core}\" auf."

def _trend_summary(subcat, defect, n, titles, causes):
    phrases = _keywords_phrases(titles + causes, top_k=6)
    examples = "; ".join([_clean_text(t)[:90] + ("â€¦" if len(_clean_text(t)) > 90 else "") for t in titles[:3] if _clean_text(t)])
    if not examples:
        examples = "â€”"
    bullets = ", ".join(phrases[:5]) if phrases else "â€”"
    return f"Die Gruppe ({subcat} â†’ {defect}) umfasst {n} Events mit Ã¤hnlicher Beschreibung/Ursache. HÃ¤ufige Muster: {bullets}. Beispiel-Titel: {examples}."

def _cluster_texts(texts, distance_threshold=0.35):
    # returns labels (deterministic) based on TFIDF cosine distance
    if len(texts) == 1:
        return np.array([0])

    # word + char tfidf (robust to typos)
    v_word = TfidfVectorizer(ngram_range=(1,2), min_df=1)
    v_char = TfidfVectorizer(analyzer="char_wb", ngram_range=(3,5), min_df=1)

    Xw = v_word.fit_transform(texts)
    Xc = v_char.fit_transform(texts)

    # weighted blend
    X = hstack([Xw.multiply(0.65), Xc.multiply(0.35)])

    sim = cosine_similarity(X)
    dist = 1 - sim

    # Agglomerative clustering with precomputed distance
    cl = AgglomerativeClustering(
        n_clusters=None,
        metric="precomputed",
        linkage="average",
        distance_threshold=distance_threshold
    )
    labels = cl.fit_predict(dist)
    return labels, sim

def generate_trends(df: pd.DataFrame):
    df = _map_headers(df)

    trends = []
    group_rollup = []  # for heatmaps

    grouped = df.groupby(["Event Subcategory (EV)", "Event Defect Code (EV)"], dropna=False, sort=True)

    for (subcat, defect), g in grouped:
        subcat = subcat if subcat else "UNSPECIFIED"
        defect = defect if defect else "UNSPECIFIED"

        # build semantic text (ONLY title + direct cause)
        sem = (g["Title (QE)"].fillna("") + " | " + g["Direct cause details (QE)"].fillna("")).map(_clean_text).tolist()
        ids = g["Name (QE)"].tolist()

        # stats for dashboard
        group_rollup.append({
            "subcategory": subcat,
            "defect_code": defect,
            "n_events_group": len(g),
        })

        # if group too small -> explicit no-trend
        if len(g) < 3:
            trends.append({
                "subcategory": subcat,
                "defect_code": defect,
                "trend_name": None,
                "trend_summary": None,
                "n_events": len(g),
                "qe_numbers": ids,
                "aggregated_titles": " | ".join([t for t in g['Title (QE)'].tolist() if t][:8]),
                "cluster_id": None,
                "is_trend": False,
            })
            continue

        labels, sim = _cluster_texts(sem, distance_threshold=0.35)

        # evaluate clusters
        g2 = g.copy()
        g2["__cluster"] = labels

        any_trend = False
        for cid, cg in g2.groupby("__cluster", sort=True):
            if len(cg) < 3:
                continue

            # cohesion gate: average pairwise similarity within cluster
            idx = cg.index.to_list()
            # sim is in order of g rows, map index->position
            pos = {i:p for p,i in enumerate(g.index.to_list())}
            pidx = [pos[i] for i in idx]
            sub_sim = sim[np.ix_(pidx, pidx)]
            # upper triangle average (excluding diagonal)
            if len(pidx) > 1:
                tri = sub_sim[np.triu_indices(len(pidx), k=1)]
                cohesion = float(np.mean(tri)) if tri.size else 0.0
            else:
                cohesion = 0.0

            if cohesion < 0.55:
                continue

            any_trend = True
            titles = cg["Title (QE)"].tolist()
            causes = cg["Direct cause details (QE)"].tolist()
            trend_name = _trend_sentence(subcat, defect, titles, causes)
            summary = _trend_summary(subcat, defect, len(cg), titles, causes)

            trends.append({
                "subcategory": subcat,
                "defect_code": defect,
                "trend_name": trend_name,
                "trend_summary": summary,
                "n_events": int(len(cg)),
                "qe_numbers": cg["Name (QE)"].tolist(),
                "aggregated_titles": " | ".join([t for t in titles if t][:12]),
                "cluster_id": int(cid),
                "is_trend": True,
                "cohesion": round(cohesion, 3),
            })

        if not any_trend:
            trends.append({
                "subcategory": subcat,
                "defect_code": defect,
                "trend_name": None,
                "trend_summary": None,
                "n_events": len(g),
                "qe_numbers": ids,
                "aggregated_titles": " | ".join([t for t in g['Title (QE)'].tolist() if t][:8]),
                "cluster_id": None,
                "is_trend": False,
            })

    out = {
        "meta": {
            "version": "demo-prototype-no-api",
            "trend_definition": ">=3 Events innerhalb Subcategory+Defect und kohÃ¤siver Textcluster (Title+DirectCause).",
            "note": "Created Date wird nicht fÃ¼rs Clustering verwendet.",
        },
        "group_rollup": group_rollup,
        "trends": trends,
    }
    return out


# -----------------------------
# UI
# -----------------------------

st.set_page_config(page_title="Deviations Trending MVP", page_icon="ðŸ“Š", layout="wide")
st.title("ðŸ“Š Deviations Trending MVP")
st.caption("Upload Excel fÃ¼r Live-Analyse oder lade vorhandenes trends.json.")

# -----------------------------
# Layout tweaks (13" friendly)
# -----------------------------
st.markdown(
    """
<style>
section[data-testid="stSidebar"] { width: 340px !important; }
section[data-testid="stSidebar"] > div { padding-top: 1rem; }
div[data-testid="stMarkdownContainer"] p { white-space: normal !important; }
div[data-testid="stMarkdownContainer"] { overflow-wrap: anywhere; }
code { white-space: pre-wrap !important; }
.block-container { padding-top: 1.2rem; padding-bottom: 1.2rem; }
</style>
    """,
    unsafe_allow_html=True,
)


mode = st.radio("Modus", ["Live-Analyse (Excel Upload)", "Repository-Modus (trends.json)"], horizontal=True)

data = None
df_events = None

if mode == "Live-Analyse (Excel Upload)":
    up = st.file_uploader("Excel (.xlsx) hochladen", type=["xlsx"])
    if up is None:
        st.info("Bitte eine Excel-Datei hochladen, um die Analyse live zu starten.")
        st.stop()
    # read first visible sheet
    df_in = pd.read_excel(up, sheet_name=0)
    data = generate_trends(df_in)
else:
    p = Path("trends.json")
    upj = st.file_uploader("Optional: trends.json hochladen", type=["json"])
    if upj is not None:
        data = json.load(upj)
    elif p.exists():
        data = json.loads(p.read_text(encoding="utf-8"))
    else:
        st.warning("Kein trends.json gefunden. Nutze den Live-Upload oder lege trends.json im Repo-Root ab.")
        st.stop()

# download json
st.download_button(
    "â¬‡ï¸ trends.json herunterladen",
    data=json.dumps(data, ensure_ascii=False, indent=2).encode("utf-8"),
    file_name="trends.json",
    mime="application/json",
)

trends = pd.DataFrame(data.get("trends", []))
roll = pd.DataFrame(data.get("group_rollup", []))

# Normalize for display
trends["subcategory"] = trends["subcategory"].fillna("UNSPECIFIED")
trends["defect_code"] = trends["defect_code"].fillna("UNSPECIFIED")
trends["is_trend"] = trends["is_trend"].fillna(False)
trends["cohesion"] = pd.to_numeric(trends.get("cohesion"), errors="coerce")

# Filters
st.sidebar.header("Filter")
subcats = ["(alle)"] + sorted(trends["subcategory"].unique().tolist())
defects = ["(alle)"] + sorted(trends["defect_code"].unique().tolist())

sel_sub = st.sidebar.selectbox("Event Subcategory", subcats)
sel_def = st.sidebar.selectbox("Event Defect Code", defects)
min_cohesion = st.sidebar.slider("Similarity (KohÃ¤sion) â€” Mindestwert", 0.40, 0.90, 0.55, 0.01)
search = st.sidebar.text_input("Textsuche (Trendname/Summary/Titel)")

f = trends.copy()
if sel_sub != "(alle)":
    f = f[f["subcategory"] == sel_sub]
if sel_def != "(alle)":
    f = f[f["defect_code"] == sel_def]

# only keep actual trends for trend list view; non-trend shown in group drilldown
f_trends = f[f["is_trend"] == True].copy()
f_trends = f_trends[f_trends["cohesion"].fillna(0) >= float(min_cohesion)]

if search.strip():
    s = search.strip().lower()
    def _match(row):
        blob = " ".join([
            str(row.get("trend_name","") or ""),
            str(row.get("trend_summary","") or ""),
            str(row.get("aggregated_titles","") or ""),
        ]).lower()
        return s in blob
    f_trends = f_trends[f_trends.apply(_match, axis=1)]

# KPI row
c1, c2, c3, c4 = st.columns(4)
c1.metric("Trends (gefiltert)", int(len(f_trends)))
c2.metric("Events in Trends", int(f_trends["n_events"].sum()) if not f_trends.empty else 0)
c3.metric("Gruppen (gesamt)", int(len(roll)) if not roll.empty else 0)
c4.metric("Ã˜ Similarity", round(float(f_trends["cohesion"].mean()),3) if (not f_trends.empty and "cohesion" in f_trends) else 0)

st.divider()

# Charts with drilldown selection
st.subheader("ðŸ”¥ Wo liegen die grÃ¶ÃŸten Probleme?")

sel = alt.selection_point(fields=["subcategory","defect_code","trend_name"], empty=True, name="pick")

bar_df = f_trends.sort_values("n_events", ascending=False).head(30)
bar_df = bar_df.copy()
bar_df["trend_label"] = bar_df["trend_name"].fillna("").map(lambda s: (s[:70] + "â€¦") if len(s) > 70 else s)
chart_h = max(260, 22 * len(bar_df))
if bar_df.empty:
    st.info("Keine Trends fÃ¼r diese Filterkombination gefunden.")
else:
    bar = alt.Chart(bar_df).mark_bar().encode(
        x=alt.X("n_events:Q", title="Anzahl Events im Trend"),
        y=alt.Y("trend_label:N", sort="-x", title="Trend"),
        tooltip=["subcategory","defect_code","n_events","cohesion","trend_name"]
    ).add_params(sel).properties(height=chart_h)
    st.altair_chart(bar, use_container_width=True)

# Heatmap: total events per group (from rollup)
st.subheader("ðŸ“Œ Heatmap: Event-Last pro Gruppe (Subcategory Ã— Defect)")
if roll.empty:
    st.info("Keine Gruppen-Rollup Daten vorhanden.")
else:
    heat_sel = alt.selection_point(fields=["subcategory","defect_code"], empty=True, name="heat_pick")
    heat = alt.Chart(roll).mark_rect().encode(
        x=alt.X("defect_code:N", title="Defect Code"),
        y=alt.Y("subcategory:N", title="Subcategory"),
        color=alt.Color("n_events_group:Q", title="Events in Gruppe"),
        tooltip=["subcategory","defect_code","n_events_group"]
    ).add_params(heat_sel).properties(height=chart_h)
    st.altair_chart(heat, use_container_width=True)

st.divider()

# Drilldown: Group overview -> trends and events
st.subheader("ðŸ“‹ Trend-Liste (gefiltert)")
if f_trends.empty:
    st.info("Keine Trends fÃ¼r die aktuelle Filterkombination.")
else:
    tbl = f_trends[["subcategory","defect_code","n_events","cohesion","trend_name","trend_summary"]].copy()
    tbl = tbl.sort_values(["n_events","cohesion"], ascending=[False, False])
    st.dataframe(
        tbl,
        use_container_width=True,
        hide_index=True,
        column_config={
            "subcategory": st.column_config.TextColumn("Subcategory"),
            "defect_code": st.column_config.TextColumn("Defect Code"),
            "n_events": st.column_config.NumberColumn("Events", format="%d"),
            "cohesion": st.column_config.NumberColumn("Similarity", format="%.3f"),
            "trend_name": st.column_config.TextColumn("Trend (Satz)"),
            "trend_summary": st.column_config.TextColumn("Zusammenfassung"),
        },
        height=min(520, 36 + 28 * min(len(tbl), 15)),
    )

st.subheader("ðŸ§© Trends innerhalb der Gruppen + zugehÃ¶rige Events")

# Determine active selection from charts is not directly available as value in Streamlit,
# so we provide explicit selection widgets for a clean demo flow.
colA, colB = st.columns([2,2])
with colA:
    drill_sub = st.selectbox("Drilldown Subcategory", ["(wÃ¤hle)"] + sorted(trends["subcategory"].unique().tolist()))
with colB:
    drill_def = st.selectbox("Drilldown Defect Code", ["(wÃ¤hle)"] + sorted(trends["defect_code"].unique().tolist()))

if drill_sub != "(wÃ¤hle)" and drill_def != "(wÃ¤hle)":
    g_all = trends[(trends["subcategory"] == drill_sub) & (trends["defect_code"] == drill_def)].copy()

    # split into trend clusters and non-trend bucket
    g_tr = g_all[g_all["is_trend"] == True].sort_values(["n_events"], ascending=False)
    g_nt = g_all[g_all["is_trend"] == False]

    st.markdown(f"**Gruppe:** `{drill_sub} â†’ {drill_def}`")
    st.write(f"Trends in Gruppe: **{len(g_tr)}**")

    if g_tr.empty:
        st.info("FÃ¼r diese Gruppe wurde kein wiederkehrender Trend identifiziert (oder Cluster < Schwellenwert).")
        # still show raw events if present in non-trend entry
        if not g_nt.empty:
            qe = g_nt.iloc[0].get("qe_numbers", [])
            st.write(f"Events in Gruppe: **{len(qe)}**")
            st.code("\n".join(qe[:200]))
    else:
        for _, row in g_tr.iterrows():
            tname = row["trend_name"]
            n = int(row["n_events"])
            cohesion = row.get("cohesion", None)
            header = f"{tname}  (n={n}" + (f", cohesion={cohesion}" if cohesion is not None else "") + ")"
            with st.expander(header if len(header)<=120 else header[:120]+"â€¦", expanded=True):
                st.write(row.get("trend_summary") or "")
                qe = row.get("qe_numbers", [])
                st.write(f"**QE Numbers ({len(qe)}):**")
                st.code("\n".join(qe))

                # show event details table if we can reconstruct from aggregated titles only in JSON
                # In live mode we can compute detailed df; in repo mode trends.json may not carry per-row fields.
                st.caption("Hinweis: FÃ¼r eine vollstÃ¤ndige Event-Tabelle (Title/Cause/Date) bleibt der Live-Upload-Modus am aussagekrÃ¤ftigsten.")
                st.write("**Aggregierte Titel (Auszug):**")
                st.write(row.get("aggregated_titles") or "â€”")

else:
    st.info("WÃ¤hle Subcategory und Defect Code fÃ¼r den Drilldown.")

st.divider()
with st.expander("ðŸ”Ž Raw JSON Preview", expanded=False):
    st.json(data)
